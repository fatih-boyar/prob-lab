[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2025 Fatih Boyar\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nEXCEPTIONS FOR WRITTEN CONTENT:\nAll non-code content (including text, images, and philosophy notes) is licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fatih Boyar",
    "section": "",
    "text": "I am a PhD student in Operations and Information Management at Bogazici University. I am curious about maths, probability, statistics, and the logic of science.\nHere, I take notes to myself in my learning journey, experiments, and simulations."
  },
  {
    "objectID": "index.html#recent-notes",
    "href": "index.html#recent-notes",
    "title": "Fatih Boyar",
    "section": "Recent Notes",
    "text": "Recent Notes"
  },
  {
    "objectID": "posts/NHST_wrong/index.html",
    "href": "posts/NHST_wrong/index.html",
    "title": "Showing Your Right Ear with Your Left Hand",
    "section": "",
    "text": "This post mainly follows Bernoulli’s Fallacy (2021) pg. 9. In this post, I’m trying to understand and explain what is wrong with the logic of Null Hypothesis Significance Testing (NHST) without even using alternative solutions like Bayes’ theorem. I want to examine these errors on their own terms, rather than siding with a rival camp, which would shift the burden of proof onto the foundations of that new method. I will save that burden for the long run.\nMost students, myself included, feel like their first statistics course is needlessly difficult, like taking a maze to cross the street. There is a Turkish idiom that perfectly captures this feeling: “showing your right ear with your left hand” (or vice versa). It describes the act of reaching over your head to touch your ear when you could have simply used the right hand. Choosing a harder way over a direct one. NHST feels exactly like this, because that is literally what it does.\n\n\n\nShowing your right ear with your left hand (Generated using Gemini)\n\n\nThe industry standard, Null Hypothesis Significance Testing (NHST) answers only this question:\n\n“Given a (null) hypothesis, what is the probability of observing data as extreme as this (or more extreme)?”\n\nFor an example:\nI have a coffee shop with 2 different bean options “Premium Roast” and “Budget Roast”. I want to know if customers could tell the difference.\n\nHypothesis: Let’s first assume this: “Customers cannot taste the difference, when they pick a cup for premium (or budget), they are guessing randomly (50/50)”\nData: I conducted a blind test (sampled from a universe of all the customers I have). Some ratio of customers pick the Premium Roast as the better one.\nAnswer: How extreme is this data given the hypothesis?\n\nThe process simply ends here. It gives us the probability of the observation (data) under the assumption (hypothesis). This is a one way logical arrow:\n\\[\n\\text{Hypothesis} \\to \\text{Data}\n\\]\nThe math ends here. The calculation itself is just a probability statement about data. However, after the end, we tend to jump to some other logical conclusions because first it just vaguely leads there, and secondly what am I going to do with probabilities of data? I am after the hypotheses.\nThe huge logical leap as follows:\n\nJump: If it is a very very low probability, then something interesting going on. Customers are not just guessing, they can tell the difference.\n\nNow the arrow simply flipped:\n\\[\n\\text{Data} \\to \\text{Hypothesis}\n\\]\nBut there is no logical reason to simply flip this arrow. Cohen (1994) explains in his paper called “The earth is round (p&lt;.05)”\n\nWhat’s wrong with NHST? Well, among many other things it does not tell us what we want to know, and we so much want to know what we want to know that, out of desperation, we nevertheless believe that it does! What we want to know is “Given these data what is the probability that \\(H_0\\) is true?” But as most of us know, what it tells us is “Given that \\(H_0\\) is true, what is the probability of these (or more extreme) data?” These are not the same.\n\nThis is called psychosemantic trap (Jeffreys 1933). Read these statements:\n\n“We are unlikely to observe an estimate of \\(x\\) far from its true value.”\n\nvs.\n\n“It is unlikely that the true value of \\(x\\) is far from what we observed”\n\nor in our coffee shop example:\n\n“Assuming the customers are guessing randomly, it is unlikely that we would observe a success rate far from 50%”\n\nvs.\n\n“Given that we observed a success rate far from 50%, it is unlikely that the customers are guessing randomly”\n\nThis feels just like an optical illusion, even if you put so much effort in, they intertwine again. It is very hard to see that they are not the same. To see it clearly, let’s try an absurdly extreme approach into our coffee shop:\n\n“I chose a customer who claims that she is psyhic. She is either a psychic or not.\nAssuming that she is not psychic, It is very unlikely (0.01, which is p&lt;0.05) that we would observe her guessing the coffee bean correctly 6 times in a row.\nShe guessed it correctly 6 times in a row.\nIf she is not psychic, this would happen only 0.01 of the time.”\n\nvs. \n\n“Given that we observed her guessing correctly 6 times in a row, there is only a 0.01 probability that she is not psychic.”\n\nWhen put it this way, it easier to see that they are not the same. And there are many arguments against this type of experiment setting but it is just the same illogical arrow flip. If we cannot reverse in this absurd example, than we cannot do it in the previous ones.\nIf you think that this is just an absurd thought experiment, lo and behold: almost exactly the same thing happened in a reputable journal in 2011. Daryl Bem from Cornell University published a paper (2011) arguing that college students have precognition (ESP), meaning that they can feel the future. In the study, students were shown two curtains on a computer screen. Behind one was a blank wall, behind the other was an image. The students had to guess where the image was. Surprisingly, when the images were erotic, students guessed correctly 53.1% of the time. This corresponded to a p-value of 0.01. Rejecting the null hypothesis, and flipping the logical arrow, he concluded that students actually have psychic powers. He did not fake the data or something, he was simply following the standard recipe correctly to make a publication.\nAll things considered, I actually have some respect for the journal and the writer in this event. They were loyal to their methods and did not bend the rules just because the conclusion was ridiculous. They were honest in what they were doing and ready to face the outcome. I once heard a professor say he rejected a submission because all the null hypotheses were conveniently rejected with low p-values (all the stars aligned). The argument was that it looked fishy, in the real world, you get messy results. But this leads us to question how you can trust any particular result with very low p-value. Isn’t that a bit too convenient as well?\nThe logical fallacy of the wrong arrow explains why p-values are so often misinterpreted. Many try to escape this trap by relying on Confidence Intervals, believing they offer a clearer picture of the truth. Unfortunately, they are built on the exact same broken logic. In my next post, I plan to run a simulation to demonstrate that a 95% confidence interval does not mean we are 95% confident in the result. In fact, it is just another instance of showing our right ear with our left hand.\n\n\n\n\nReferences\n\nBem, Daryl J. 2011. “Feeling the Future: Experimental Evidence for Anomalous Retroactive Influences on Cognition and Affect.” Journal of Personality and Social Psychology 100 (3): 407.\n\n\nClayton, Aubrey. 2021. Bernoulli’s Fallacy: Statistical Illogic and the Crisis of Modern Science. Columbia University Press.\n\n\nCohen, Jacob. 1994. “The Earth Is Round (p&lt;. 05).” American Psychologist 49 (12): 997.\n\n\nJeffreys, Harold. 1933. “Probability, Statistics, and the Theory of Errors.” Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character 140 (842): 523–35."
  },
  {
    "objectID": "posts/opinions/index.html",
    "href": "posts/opinions/index.html",
    "title": "Opinions to Age",
    "section": "",
    "text": "I will use this page to vent my opinions, hot-takes, and rants. Later, I will see if they aged like wine or milk."
  },
  {
    "objectID": "posts/opinions/index.html#the-learning-and-improving-side-of-social-media-is-mostly-noise-and-we-are-overwhelmed-by-it",
    "href": "posts/opinions/index.html#the-learning-and-improving-side-of-social-media-is-mostly-noise-and-we-are-overwhelmed-by-it",
    "title": "Opinions to Age",
    "section": "The learning and improving side of social media is mostly noise, and we are overwhelmed by it",
    "text": "The learning and improving side of social media is mostly noise, and we are overwhelmed by it\nJanuary 15, 2026\nI used to find YouTube tremendously helpful for learning and being exposed to new ideas and perspectives. It had the same unfortunate fate of ‘engagement maxxing’ algorithmic change with the other media. Now it feels like every bit of new information, any advice, dos and don’ts are overwhelming, too much, sloppy, unimportant, and further annoying. I’m not an expert of how these algorithms work but I guess that the channel owners (or creators in more contemporary jargon) are forced to not only make new things consistently, but more rapidly. However, knowledge (or say useful, helpful or novel things worth sharing) does not accumulate with the expected rapid pace. Therefore, apart from shortening the contents made, we are exposed to more and more trivial, marginal, or worse, untested things. So we should develop criterion that examines how much a channel (or creator) keep its feet on the ground vs. makes trivial, unimportant things. The measure can be something like a mix of ‘skin in the game’ and ‘walking the walk’ as in NN Taleb’s term. Take the portfolio of a creator and ask how much of those things you told us are still important to you, do or use them on a daily basis, stand by them to this day, or made a habit of them so it is still part of you, and will you be subject to the possible downsides with your audience if you are wrong.\nFor an example. I remember this thing called ‘Coffee Nap’. The idea is you drink coffee first, then do a power nap right after (for 15-20 min). Meanwhile the caffeine works its way so when you wake up you both get the benefits of the nap and the caffeine simultaneously. Apart from taking gifts of nature such as a nice cup of coffee and turning it to an extremely utilized commodity for ‘efficiency’, let’s ask the people who advocated this idea when it was trendy: do you still do it? You have a 25-minute ‘video essay’ on it from 4 years ago, tell me how this great innovation improved your life. I am pretty sure that the percentage of people who made a habit of this strategy that offers very marginal advantage.\nIt’s a good strategy to eliminate the unfavorable examples. It can be used also for the good ones. They typically live by and be transparent with their opinions, advice, rules, and overall work they present. Without advocating for their ideas to be good or correct in any sense, I can exemplify some of the YouTubers off the top of my head: Kneesovertoesguy, WhatIveLearned (Joseph Everett), movementbydavid. These people live by their own advice and be subject to them if mistaken. A really unfavorable example could be TedX talks with unrelated, untested, unimportant stuff poured at our heads.\nThis might sound like a trivial problem to focus on in these days, but I believe that in some part this is the reason why we are overwhelmed and got sick of everything that is conveyed to us. It’s just too much, can’t distinguish good from bad, get irritated, thus shut down our percepctions altogether."
  },
  {
    "objectID": "posts/missing_priors/index.html",
    "href": "posts/missing_priors/index.html",
    "title": "Why Statistics ‘Proved’ Psychic Powers",
    "section": "",
    "text": "In this post, I will explore the famous Bayes’ Theorem, avoiding the jargon of any specific statistical camp. Instead, I will use a basic derivation to arrive at the theorem naturally.\nIn my previous post, I attempted to explain that the probability of the data given the hypothesis is not the same thing as probability of the hypothesis given the data.\n\\[\n\\text{Hypothesis} \\to \\text{Data} \\ne \\text{Data} \\to \\text{Hypothesis}\n\\]\nTo be more precise, using mathematical notation:\n\\[\n\\mathbb{P}(D \\mid H) \\ne \\mathbb{P}(H \\mid D)\n\\]\nIn real life, we are after \\(\\mathbb{P}(H \\mid D)\\) (probability of the hypothesis given data) meaning how reasonable it is for this hypothesis to be true, based on the data at hand. But NHST gives us only \\(\\mathbb{P}(D \\mid H)\\) (probability of the data given the hypothesis) meaning how likely we would get this data if we assume the hypothesis to be true (or how extreme would that be because we are trying to reject the null hypothesis, showing our right ear with our left hand).\nTo see why this matters, let’s derive what we actually want: \\(\\mathbb{P}(H \\mid D)\\). By the definition of conditional probability:\n\\[ \\mathbb{P}(H \\mid D) = \\frac{\\mathbb{P}(H \\text{ and } D)}{\\mathbb{P}(D)} \\tag{1} \\]\nWe don’t know the top part (\\(\\mathbb{P}(H \\text{ and } D)\\)) directly. But we can calculate it using the information we do have (the logical arrow from Hypothesis to Data). The probability of the Hypothesis and Data happening together is simply the probability of the Hypothesis being true in the first place, multiplied by the probability of the Data given that hypothesis (this is called multiplication rule):\n\\[ \\mathbb{P}(H \\text{ and } D) = \\mathbb{P}(H) \\times \\mathbb{P}(D \\mid H) \\tag{2} \\]\nIf we substitute (2) back into (1), we get the full picture:\n\\[ \\mathbb{P}(H \\mid D) = \\frac{\\mathbb{P}(H) \\times \\mathbb{P}(D \\mid H)}{\\mathbb{P}(D)} \\]\nLook at the numerator. The result depends on two things being multiplied: 1. \\(\\mathbb{P}(D \\mid H)\\): How likely the data is (What the study calculated). 2. \\(\\mathbb{P}(H)\\): The probability of the hypothesis being true in the first place (The Prior).\nThis reveals the fatal flaw in the psychic study. Even if the data was likely (\\(\\mathbb{P}(D \\mid H)\\) is high), the prior probability of a college student having psychic powers (\\(\\mathbb{P}(H)\\)) is infinitesimally small. When you multiply a big number by near-zero, you get near-zero. NHST fails because it ignores this multiplication.\nBelow is a demonstration of how \\(\\mathbb{P}(H)\\) changes the result. The red line corresponds to ignoring the priors, or taking it 50%. The dotted line changes with the prior probability parameter. Notice how much evidence (consecutive guesses) is needed to reach 95% certainty when we assign a very low probability to someone being a psychic. While the red line just crosses over it after 4 guesses.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nfrom shiny import App, ui, render\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Logic for Bayesian Updating\ndef calculate_posterior(n_successes, prior):\n    # Likelihood of data given Psychic (Assume perfect psychic, accuracy = 1.0)\n    # If they are psychic, probability of n correct guesses is 1^n = 1.\n    likelihood_psychic = 1.0\n    \n    # Likelihood of data given Random Guesser (accuracy = 0.5)\n    # If random, probability of n correct guesses is 0.5^n.\n    likelihood_random = 0.5 ** n_successes\n    \n    # Bayes Theorem: P(Psychic | Data)\n    numerator = likelihood_psychic * prior\n    denominator = (likelihood_psychic * prior) + (likelihood_random * (1 - prior))\n    \n    return numerator / denominator\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.h4(\"The Skeptic's Settings\"),\n            ui.input_slider(\"exponent\", \"Prior Probability (1 in 10^x)\", min=4, max=10, value=6),\n            ui.output_text_verbatim(\"prior_text\"),\n            ui.hr(),\n            ui.input_slider(\"max_trials\", \"Max Guesses to Observe\", min=5, max=50, value=25),\n            ui.markdown(\n                \"\"\"\n                **Red Line:** Starts at 50% (Naive).  \n                **Blue Line:** Starts at your chosen Prior.\n                \"\"\"\n            )\n        ),\n        ui.card(\n            ui.output_plot(\"bayes_plot\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    \n    @output\n    @render.text\n    def prior_text():\n        val = 10**input.exponent()\n        return f\"Skeptic's Prior: 1 in {val:,}\"\n\n    @output\n    @render.plot\n    def bayes_plot():\n        # 1. Setup Data\n        n_trials = input.max_trials()\n        x_axis = np.arange(0, n_trials + 1)\n        \n        # 2. Calculate Naive Observer (Prior = 0.5)\n        prior_naive = 0.5\n        y_naive = [calculate_posterior(n, prior_naive) for n in x_axis]\n        \n        # 3. Calculate Skeptic Observer (User Input)\n        prior_skeptic = 1 / (10**input.exponent())\n        y_skeptic = [calculate_posterior(n, prior_skeptic) for n in x_axis]\n\n        # 4. Plotting\n        fig, ax = plt.subplots()\n        \n        # Plot Naive\n        ax.plot(x_axis, y_naive, color=\"#e74c3c\", linewidth=2, label=f\"Naive Prior (0.5)\")\n        \n        # Plot Skeptic\n        ax.plot(x_axis, y_skeptic, color=\"#2c3e50\", linewidth=2, linestyle=\"--\", label=f\"Skeptic Prior (1 in 10^{input.exponent()})\")\n        \n        # Formatting\n        ax.set_ylim(-0.05, 1.05)\n        ax.set_xlabel(\"Consecutive Correct Guesses\")\n        ax.set_ylabel(\"Probability they are Psychic\")\n        ax.set_title(\"How P(H) Changes the Outcome\")\n        ax.legend(loc=\"lower right\", fontsize=\"small\")\n        ax.grid(True, linestyle=':', alpha=0.6)\n        \n        # Add a threshold line for 95% certainty\n        ax.axhline(y=0.95, color='green', linestyle=':', alpha=0.5)\n        ax.text(0, 0.96, \"95% Certainty Threshold\", color='green', fontsize=8)\n\n        return fig\n\napp = App(app_ui, server)\nAssume we toss a fair coin many times. ‘Fair’ means the probability of heads or tails is 50/50. This corresponds to the Null Hypothesis: a world where no one is psychic and everyone is just guessing.\nWatch how a streak of 6 consecutive tails appears. We know the coin is fair, so this streak is pure chance. But if we stopped the experiment right at that moment, it would look like statistically significant evidence.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nfrom shiny import App, ui, render\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport numpy as np\n\n# Logic to generate flips and find streaks\ndef generate_and_highlight(n_flips):\n    # 0 = Heads, 1 = Tails\n    flips = np.random.choice([0, 1], size=n_flips)\n    \n    # Create a color map array: 0=Heads, 1=Tails, 2=Streak\n    visual_map = flips.copy()\n    \n    # Find sequences of 6 or more 1s (Tails)\n    streak_len = 6\n    for i in range(len(flips) - streak_len + 1):\n        # Check if this slice is all 1s\n        if np.all(flips[i : i+streak_len] == 1):\n            # Mark these indices as 2 (Red)\n            visual_map[i : i+streak_len] = 2\n            \n    return flips, visual_map\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.h4(\"Coin Toss Settings\"),\n            ui.input_slider(\"n_tosses\", \"Number of Tosses\", min=100, max=2000, value=500, step=50),\n            ui.input_action_button(\"rerun\", \"Flip Again!\"),\n            ui.hr(),\n            ui.h5(\"Legend\"),\n            ui.markdown(\n                \"\"\"\n                **Light Gray:** Heads  \n                **Dark Gray:** Tails  \n                **&lt;span style='color: #e74c3c'&gt;Red:&lt;/span&gt;** 6+ Tails in a row\n                \"\"\"\n            ),\n        ),\n        ui.card(\n            ui.output_plot(\"grid_plot\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    \n    @output\n    @render.plot\n    def grid_plot():\n        input.rerun() # React to button\n        n = input.n_tosses()\n        \n        # Generate Data\n        flips, v_map = generate_and_highlight(n)\n        \n        # Calculate grid dimensions to make it look like a square/rectangle\n        # We fix the width to make it readable (e.g., 25 columns)\n        cols = 25\n        rows = (n // cols) + (1 if n % cols &gt; 0 else 0)\n        \n        # Pad the array with -1 (empty) to fit the grid shape\n        pad_size = (rows * cols) - n\n        v_map_padded = np.pad(v_map, (0, pad_size), constant_values=-1)\n        grid = v_map_padded.reshape((rows, cols))\n        \n        # Plotting\n        fig, ax = plt.subplots(figsize=(6, 8))\n        \n        # Define Custom Colors\n        # -1: White (Empty/Background)\n        # 0: Light Gray (Heads)\n        # 1: Dark Gray (Tails)\n        # 2: Red (Streak)\n        cmap = mcolors.ListedColormap(['white', '#ecf0f1', '#34495e', '#e74c3c'])\n        bounds = [-1.5, -0.5, 0.5, 1.5, 2.5]\n        norm = mcolors.BoundaryNorm(bounds, cmap.N)\n        \n        ax.imshow(grid, cmap=cmap, norm=norm, aspect='equal')\n        \n        # Clean up chart\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"Visualizing {n} Coin Tosses\", fontsize=14)\n\n        return fig\n\napp = App(app_ui, server)\nIn the end, the error in the psychic study (and in NHST generally) wasn’t a calculation mistake; it was a conceptual one. If the claim is extraordinary (like psychic powers), a \\(p\\)-value of 0.01 is simply not heavy enough to tip the scale against the massive weight of prior probability."
  },
  {
    "objectID": "posts/CI_simulation/index.html",
    "href": "posts/CI_simulation/index.html",
    "title": "Confidence Intervals",
    "section": "",
    "text": "This post is continuation in context with the previous NHST post.\n\n\n\nConfidence Intervals are not what they seem -log lady\n\n\nThe psychosemantic trap of NHST was: we calculate the probability of the data (given hypothesis), but we pretend we calculated the probability of hypothesis (given data). The same trap exists in confidence intervals also. Compare these:\n\\[\n\\text{Hypothesis} \\to \\text{Data}\n\\]\n\n“Assuming the true value is \\(x\\), there is a 95% probability that our random procedure will generate an interval that captures it.”\n\nvs.\n\\[\n\\text{Data} \\to \\text{Hypothesis}\n\\]\n\n“Given this specific interval we just calculated, there is a 95% probability that the True Value is inside it.”\n\nIn the first one, the logic requires a strict assumption: the parameter (e.g., mean) must be treated as a fixed constant. In this view, the value is set in stone. Only God (or Nature) knows it, and it does not have a probability; it just is. Since we cannot know it, the method relies on a hypothetical scenario: if we were to take infinitely many samples with the same size, these intervals would capture the true parameter 95% of the time. So approximately in 5% of the time, the true parameter would not be in these intervals at all.\nBut in the second one, the same illegal flip is made. The probability in this setting applies only to the data (intervals), which is a random variable. Under this framework, calculating the probability of a hypothesis is impossible. Because the parameter is defined as fixed, it cannot have a probability distribution. The probability is technically 1 or 0, but the math offers us no way to know which.\nTo put in other way, a 95% confidence interval is “The set of all Hypotheses (\\(H_0\\)) that we would not reject at the \\(p&gt;0.05\\) level”.\nLet’s take our coffee shop example. We want to know the true percentage of people who can distinguish premium beans to budget beans.\n\n\n\n\n\n\n\n\nGuess\nTest\nResult\n\n\n\n\nIs the true percentage 50%?\nRun a test assuming \\(H_0 = 50\\%\\)\np-value = 0.03 (significant). Then reject \\(50\\%\\)\n\n\nIs the true percentage 52%?\nRun a test assuming \\(H_0 = 52\\%\\)\np-value = 0.04 (significant). Then reject \\(52\\%\\)\n\n\nIs the true percentage 53%?\nRun a test assuming \\(H_0 = 53\\%\\)\np-value = 0.06 (not significant). Then keep \\(53\\%\\)\n\n\nIs the true percentage 60%?\nRun a test assuming \\(H_0 = 60\\%\\)\np-value = 0.08 (not significant). Then keep \\(60\\%\\)\n\n\n…\n…\n…\n\n\n\nEvery single step of this process still use the \\(\\text{Hypothesis} \\to \\text{Data}\\) direction. For every possible number from 0% to 100%, we are asking “if the truth were \\(x\\%\\), would this data be weird?”\nLet’s see in a simulation to understand the CI in action. Imagine we are sampling from our customer base and test them if they can tell the premium coffee. There are 100 confidence intervals in the simulation below. We can think of experimenting 1 time, in 100 different parallel universes. Or experimenting 100 different times with the same sample size. The hidden truth parameter is the true percentage of a customers success rate of finding the premium coffee. Or the probability of finding the premium coffee of a customer.\nThe samples generated from a binomial distribution: \\[X \\sim \\text{Bin}(n, p)\\] \\[P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\]\nThese formulas mean that X follows a binomial distribution. And the probability of X being some value k equals to that formula. The simulation creates exactly the same sample if the probability of finding out the premium coffee is equal to that parameter (p) As you see, in 95% confidence setting, several intervals do not even include the true parameter. In 90% the picture is quite worse. So take 100 papers using p-value=0.05 and expect around 5 of them doesn’t even capture the true value with their intervals Even if we ignore the illegal arrow flip.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 800\n#| components: [viewer]\n\n## file: app.py\nfrom shiny import App, ui, render\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- MINIMAL UI ---\napp_ui = ui.page_fluid(\n    ui.card(\n        ui.card_header(\"Experiment Controls\"),\n        ui.layout_columns(\n            ui.input_slider(\"true_p\", \"Hidden Truth (p)\", 0.3, 0.7, 0.5, step=0.01),\n            ui.input_slider(\"sample_size\", \"Sample Size (n)\", 10, 500, 50),\n            ui.input_radio_buttons(\"alpha\", \"Confidence\", \n                                   {\"0.01\": \"99%\", \"0.05\": \"95%\", \"0.10\": \"90%\"}, \n                                   selected=\"0.05\", inline=True),\n            ui.input_action_button(\"rerun\", \"Run\", class_=\"btn-primary w-100 mt-4\"),\n            col_widths=(4, 4, 2, 2)\n        )\n    ),\n    ui.card(\n        ui.output_plot(\"ci_plot\", height=\"500px\"),\n    )\n)\n\n# --- SERVER ---\ndef server(input, output, session):\n    @render.plot\n    def ci_plot():\n        input.rerun()\n        \n        N = 100  \n        n = input.sample_size()\n        true_p = input.true_p()\n        alpha = float(input.alpha())\n        \n        successes = np.random.binomial(n, true_p, N)\n        sample_ps = successes / n\n        \n        z_map = {0.01: 2.576, 0.05: 1.96, 0.10: 1.645}\n        z_score = z_map.get(alpha, 1.96)\n        \n        se = np.sqrt((sample_ps * (1 - sample_ps)) / n)\n        lower = sample_ps - (z_score * se)\n        upper = sample_ps + (z_score * se)\n        \n        hit = (lower &lt;= true_p) & (upper &gt;= true_p)\n        # Colors: Dark Grey for hits, Red for misses\n        colors = np.where(hit, '#555555', '#E74C3C')\n        widths = np.where(hit, 1, 2.5)\n        alphas = np.where(hit, 0.4, 1.0)\n        \n        fig, ax = plt.subplots(figsize=(10, 6), constrained_layout=True)\n        \n        # The Truth Line\n        ax.axvline(true_p, color='black', linestyle='-', linewidth=2, label=\"Truth\")\n        \n        y_pos = np.arange(N)\n        for i in range(N):\n            ax.hlines(y_pos[i], lower[i], upper[i], color=colors[i], linewidth=widths[i], alpha=alphas[i])\n        \n        ax.set_yticks([])\n        ax.set_xlim(0.2, 0.8)\n        \n        # Smaller Fonts\n        ax.set_xlabel(\"Estimated Proportion\", fontsize=10, color=\"gray\")\n        ax.set_title(f\"{N - np.sum(hit)} Intervals Missed the Truth\", fontsize=11, fontweight=\"bold\")\n        \n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        ax.spines['left'].set_visible(False)\n        ax.spines['bottom'].set_color('#CCCCCC')\n        ax.tick_params(axis='x', colors='gray')\n        \n        return fig\n\napp = App(app_ui, server)\n\nWe look at a Confidence Interval and our intuition sees a probability range for the truth. But the logic of orthodox statistics forbids this interpretation. It is a performance guarantee for a method, not a verdict on reality. Next time, like the owls in Twin Peaks, imagine the Log Lady warning you with the intervals."
  },
  {
    "objectID": "posts/learning_is_difficult/index.html",
    "href": "posts/learning_is_difficult/index.html",
    "title": "Learning is difficult",
    "section": "",
    "text": "Learning is difficult, and it doesn’t get easier. The only thing you realize is that the previous arduous steps seem surprisingly trivial in retrospect. This amazes me. It is like a paradox, the strenuous effort itself is painful and not immediately rewarding. You just remember the hardship as “huh, how hard this thing was before that I’m doing now as if nothing”. To keep learning and improving, you must have this awareness all the time, otherwise it’s just the feeling of difficulty.\nThis way of thinking lead me to understand that it is possible. When you encounter something new (a new topic or a skill), if it is very foreign to you, it may feel like it is impossible to comprehend or to achieve. But that is the only way. If it is difficult, I’m in the correct route. It may sound so trivial when I put it this way but we often miss how learning works and don’t recognize it when we avoid this hard effort. I remember when I meet with this peculiar exam for a pilot training program called DLR test. At first, I said this is just not possible for me. But day by day, improving incrementally, I passed it in the first try (later I gave up on this and thankfully not became a commercial pilot).\nI played video games since I was a kid. If you’d remember, games were more fun and they were objectively more difficult. I remember that I was grinding and pushing and keep trying when I was playing. It was just part of it. I remember that I was teaching techniques to neighbor’s kids older than me to pass a certain level in some game in Atari. Despite this childhood experience, I was struck with this famous indie game called Hollow Knight: Silksong. I said this is ridiculously hard, why do the developers torture me? I even thought some parts are just impossible for me to complete. But then, day by day you keep getting better and improve your skills. I’m amazed how parts that once made me want to throw the controller have now become so easy, only with my improvements in skills. There is the predominant advice in such games among the community: “git gud”.\n\n\n\nHornet from Silksong (Hollow Knight Fandom)\n\n\nI am not the most athletic type, maybe partly because of too much exercise in video games in my developing years and not so much in the sports. But tremendous news, it doesn’t matter too much if your goal is not become a pro. Even if you feel like inherently you are not good at something, or lack of some prior experience or talent, it is perfectly doable. I try to find things that are difficult and know that this is not only a good but the necessary indicator. At first, it was so painful to push myself in road cycling, but now I easily do a 100km in a day.\n\n\n\nMy shameless self promotion of my fitness\n\n\nWhat’s next is math. I’ve always felt inadequate. So my goal is to reach the proficiency level of a math undergrad to have what it takes to confidently discuss probability and statistics. It feels so difficult even now that it’s holding me back from starting. But let’s never ever forget that it is the only way to “git gud”."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Most published research findings are false (Ioannidis 2005). Unfortunately, the main reason isn’t a few “bad apples” engaging in p-hacking, data fabrication, or other fraudulent practices. Rather, it is due to a form of statistical blindness; specifically, the mechanical application of frequentist methods like significance testing simply because they are the industry standard.\nSince the early 2010s, we have indeed witnessed a massive issue known as the Replication Crisis. Many cornerstone studies, ranging from the social sciences to medicine, failed to replicate because they all followed the same rigid rules of statistical testing. Yet, we seem to keep applying temporary patches, such as lowering the acceptance threshold for p-values to 0.01.\nThe bridge out of this mess is actually rethinking probability at a fundamental level. It is a painful realization, mostly because it forces us to go back to the start while everyone else is still busy fighting reviewers for publications. But really, it just returns us to the perspective E.T. Jaynes pointed out: probability is not a tool for science, but the logic of science (Jaynes 2003). This isn’t something we can neglect, especially in the social sciences where statistical packages are too often treated like push-button appliances.\nSo, I decided to save the world (as every PhD student attempts to do) or die trying (which means overall, publishing a few papers). Bernoulli’s Fallacy by Aubrey Clayton (2021) greatly helped me put the pieces together. This blog documents my journey of following the Clayton’s steps, re-learning probability, running simulations, and building applets, along with some personal reflections."
  },
  {
    "objectID": "about.html#footnotes",
    "href": "about.html#footnotes",
    "title": "About",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(link to the ig page)↩︎"
  }
]