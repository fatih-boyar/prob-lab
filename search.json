[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2025 Fatih Boyar\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nEXCEPTIONS FOR WRITTEN CONTENT:\nAll non-code content (including text, images, and philosophy notes) is licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fatih Boyar",
    "section": "",
    "text": "I am a PhD student in Operations and Information Management at Bogazici University. I am curious about maths, probability, statistics, and the logic of science.\nHere, I take notes to myself in my learning journey, experiments, and simulations."
  },
  {
    "objectID": "index.html#recent-notes",
    "href": "index.html#recent-notes",
    "title": "Fatih Boyar",
    "section": "Recent Notes",
    "text": "Recent Notes"
  },
  {
    "objectID": "posts/NHST_wrong/index.html",
    "href": "posts/NHST_wrong/index.html",
    "title": "Showing Your Right Ear with Your Left Hand",
    "section": "",
    "text": "This post mainly follows Bernoulli’s Fallacy (2021) pg. 9. In this post, I’m trying to understand and explain what is wrong with the logic of Null Hypothesis Significance Testing (NHST) without even using alternative solutions like Bayes’ theorem. I want to examine these errors on their own terms, rather than siding with a rival camp, which would shift the burden of proof onto the foundations of that new method. I will save that burden for the long run.\nMost students, myself included, feel like their first statistics course is needlessly difficult, like taking a maze to cross the street. There is a Turkish idiom that perfectly captures this feeling: “showing your right ear with your left hand” (or vice versa). It describes the act of reaching over your head to touch your ear when you could have simply used the right hand. Choosing a harder way over a direct one. NHST feels exactly like this, because that is literally what it does.\n\n\n\nShowing your right ear with your left hand (Generated using Gemini)\n\n\nThe industry standard, Null Hypothesis Significance Testing (NHST) answers only this question:\n\n“Given a (null) hypothesis, what is the probability of observing data as extreme as this (or more extreme)?”\n\nFor an example:\nI have a coffee shop with 2 different bean options “Premium Roast” and “Budget Roast”. I want to know if customers could tell the difference.\n\nHypothesis: Let’s first assume this: “Customers cannot taste the difference, when they pick a cup for premium (or budget), they are guessing randomly (50/50)”\nData: I conducted a blind test (sampled from a universe of all the customers I have). Some ratio of customers pick the Premium Roast as the better one.\nAnswer: How extreme is this data given the hypothesis?\n\nThe process simply ends here. It gives us the probability of the observation (data) under the assumption (hypothesis). This is a one way logical arrow:\n\\[\n\\text{Hypothesis} \\to \\text{Data}\n\\]\nThe math ends here. The calculation itself is just a probability statement about data. However, after the end, we tend to jump to some other logical conclusions because first it just vaguely leads there, and secondly what am I going to do with probabilities of data? I am after the hypotheses. And this is not a thing for uneducated or amateurs, it’s the way of most scientists.\nThe huge logical leap as follows:\n\nJump: If it is a very very low probability, then something interesting going on. Customers are not just guessing, they can tell the difference.\n\nNow the arrow simply flipped:\n\\[\n\\text{Data} \\to \\text{Hypothesis}\n\\]\nBut there is no logical reason to simply flip this arrow. Cohen (1994) explains in his paper called “The earth is round (p&lt;.05)”\n\nWhat’s wrong with NHST? Well, among many other things it does not tell us what we want to know, and we so much want to know what we want to know that, out of desperation, we nevertheless believe that it does! What we want to know is “Given these data what is the probability that \\(H_0\\) is true?” But as most of us know, what it tells us is “Given that \\(H_0\\) is true, what is the probability of these (or more extreme) data?” These are not the same.\n\nThis is called psychosemantic trap (Jeffreys 1933). Read these statements:\n\n“We are unlikely to observe an estimate of \\(x\\) far from its true value.”\n\nvs.\n\n“It is unlikely that the true value of \\(x\\) is far from what we observed”\n\nor in our coffee shop example:\n\n“Assuming the customers are guessing randomly, it is unlikely that we would observe a success rate far from 50%”\n\nvs.\n\n“Given that we observed a success rate far from 50%, it is unlikely that the customers are guessing randomly”\n\nThis feels just like an optical illusion, even if you put so much effort in, they intertwine again. It is very hard to see that they are not the same. To see it clearly, let’s try an absurdly extreme approach into our coffee shop:\n\n“I chose a customer who claims that she is psyhic. But of course I don’t believe that she is psychic.\nAssuming that she is not psychic, It is very unlikely (0.01, which is p&lt;0.05) that we would observe her guessing the coffee bean correctly 6 times in a row.\nShe guessed it correctly 6 times in a row.\nIf she is not psychic, this would happen only 0.01 of the time.”\n\nvs. \n\n“Given that we observed her guessing correctly 6 times in a row, there is only a 0.01 probability that she is not psychic.”\n\nWhen put it this way, it easier to see that they are not the same. And there are many arguments against this type of experiment setting but it is just the same illogical arrow flip. If we cannot reverse in this absurd example, than we cannot do it in the previous ones.\nIf you think that this is just an absurd thought experiment, lo and behold: almost exactly the same thing happened in a reputable journal in 2011. Daryl Bem from Cornell University published a paper (2011) arguing that college students have precognition (ESP), meaning that they can feel the future. In the study, students were shown two curtains on a computer screen. Behind one was a blank wall, behind the other was an image. The students had to guess where the image was. Surprisingly, when the images were erotic, students guessed correctly 53.1% of the time. This corresponded to a p-value of 0.01. Rejecting the null hypothesis, and flipping the logical arrow, he concluded that students actually have psychic powers. He did not fake the data or something, he was simply following the standard recipe correctly to make a publication.\nAll things considered, I actually have some respect for the journal and the writer in this event. They were loyal to their methods and did not bend the rules just because the conclusion was ridiculous. They were honest in what they were doing and ready to face the outcome. I once heard a professor say he rejected a submission because all the null hypotheses were conveniently rejected with low p-values (all the stars aligned). The argument was that it looked fishy, in the real world, you get messy results. But this leads us to question how you can trust any particular result with very low p-value. Isn’t that a bit too convenient as well?\nThe logical fallacy of the wrong arrow explains why p-values are so often misinterpreted. Many try to escape this trap by relying on Confidence Intervals, believing they offer a clearer picture of the truth. Unfortunately, they are built on the exact same broken logic. In my next post, I plan to run a simulation to demonstrate that a 95% confidence interval does not mean we are 95% confident in the result. In fact, it is just another instance of showing our right ear with our left hand.\n\n\n\n\nReferences\n\nBem, Daryl J. 2011. “Feeling the Future: Experimental Evidence for Anomalous Retroactive Influences on Cognition and Affect.” Journal of Personality and Social Psychology 100 (3): 407.\n\n\nClayton, Aubrey. 2021. Bernoulli’s Fallacy: Statistical Illogic and the Crisis of Modern Science. Columbia University Press.\n\n\nCohen, Jacob. 1994. “The Earth Is Round (p&lt;. 05).” American Psychologist 49 (12): 997.\n\n\nJeffreys, Harold. 1933. “Probability, Statistics, and the Theory of Errors.” Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character 140 (842): 523–35."
  },
  {
    "objectID": "posts/CI_simulation/index.html",
    "href": "posts/CI_simulation/index.html",
    "title": "Confidence Intervals",
    "section": "",
    "text": "This post is continuation in context with the previous NHST post.\n\n\n\nConfidence Intervals are not what they seem -log lady\n\n\nThe psychosemantic trap of NHST was: we calculate the probability of the data (given hypothesis), but we pretend we calculated the probability of hypothesis (given data). The same trap exists in confidence intervals also. Compare these:\n\\[\n\\text{Hypothesis} \\to \\text{Data}\n\\]\n\n“Assuming the true value is \\(x\\), there is a 95% probability that our random procedure will generate an interval that captures it.”\n\nvs.\n\\[\n\\text{Data} \\to \\text{Hypothesis}\n\\]\n\n“Given this specific interval we just calculated, there is a 95% probability that the True Value is inside it.”\n\nIn the first one, the logic requires a strict assumption: the parameter (e.g., mean) must be treated as a fixed constant. In this view, the value is set in stone. Only God (or Nature) knows it, and it does not have a probability; it just is. Since we cannot know it, the method relies on a hypothetical scenario: if we were to take infinitely many samples with the same size, these intervals would capture the true parameter 95% of the time. So approximately in 5% of the time, the true parameter would not be in these intervals at all.\nBut in the second one, the same illegal flip is made. The probability in this setting applies only to the data (intervals), which is a random variable. Under this framework, calculating the probability of a hypothesis is impossible. Because the parameter is defined as fixed, it cannot have a probability distribution. The probability is technically 1 or 0, but the math offers us no way to know which.\nTo put in other way, a 95% confidence interval is “The set of all Hypotheses (\\(H_0\\)) that we would not reject at the \\(p&gt;0.05\\) level”.\nLet’s take our coffee shop example. We want to know the true percentage of people who can distinguish premium beans to budget beans.\n\n\n\n\n\n\n\n\nGuess\nTest\nResult\n\n\n\n\nIs the true percentage 50%?\nRun a test assuming \\(H_0 = 50\\%\\)\np-value = 0.03 (significant). Then reject \\(50\\%\\)\n\n\nIs the true percentage 52%?\nRun a test assuming \\(H_0 = 52\\%\\)\np-value = 0.04 (significant). Then reject \\(52\\%\\)\n\n\nIs the true percentage 53%?\nRun a test assuming \\(H_0 = 53\\%\\)\np-value = 0.06 (not significant). Then keep \\(53\\%\\)\n\n\nIs the true percentage 60%?\nRun a test assuming \\(H_0 = 60\\%\\)\np-value = 0.08 (not significant). Then keep \\(60\\%\\)\n\n\n…\n…\n…\n\n\n\nEvery single step of this process still use the \\(\\text{Hypothesis} \\to \\text{Data}\\) direction. For every possible number from 0% to 100%, we are asking “if the truth were \\(x\\%\\), would this data be weird?”\nLet’s see in a simulation to understand the CI in action. Imagine we are sampling from our customer base and test them if they can tell the premium coffee. There are 100 confidence intervals in the simulation below. We can think of experimenting 1 time, in 100 different parallel universes. Or experimenting 100 different times with the same sample size. The hidden truth parameter is the true percentage of a customers success rate of finding the premium coffee. Or the probability of finding the premium coffee of a customer.\nThe samples generated from a binomial distribution: \\[X \\sim \\text{Bin}(n, p)\\] \\[P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\]\nThese formulas mean that X follows a binomial distribution. And the probability of X being some value k equals to that formula. The simulation creates exactly the same sample if the probability of finding out the premium coffee is equal to that parameter (p) As you see, in 95& confidence setting, several intervals do not even include the true parameter. In 90% the picture is quite worse. So take 100 papers using p-value=0.05 and expect around 5 of them doesn’t even capture the true value with their intervals Even if we ignore the illegal arrow flip.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 800\n#| components: [viewer]\n\n## file: app.py\nfrom shiny import App, ui, render\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- MINIMAL UI ---\napp_ui = ui.page_fluid(\n    ui.card(\n        ui.card_header(\"Experiment Controls\"),\n        ui.layout_columns(\n            ui.input_slider(\"true_p\", \"Hidden Truth (p)\", 0.3, 0.7, 0.5, step=0.01),\n            ui.input_slider(\"sample_size\", \"Sample Size (n)\", 10, 500, 50),\n            ui.input_radio_buttons(\"alpha\", \"Confidence\", \n                                   {\"0.01\": \"99%\", \"0.05\": \"95%\", \"0.10\": \"90%\"}, \n                                   selected=\"0.05\", inline=True),\n            ui.input_action_button(\"rerun\", \"Run\", class_=\"btn-primary w-100 mt-4\"),\n            col_widths=(4, 4, 2, 2)\n        )\n    ),\n    ui.card(\n        ui.output_plot(\"ci_plot\", height=\"500px\"),\n    )\n)\n\n# --- SERVER ---\ndef server(input, output, session):\n    @render.plot\n    def ci_plot():\n        input.rerun()\n        \n        N = 100  \n        n = input.sample_size()\n        true_p = input.true_p()\n        alpha = float(input.alpha())\n        \n        successes = np.random.binomial(n, true_p, N)\n        sample_ps = successes / n\n        \n        z_map = {0.01: 2.576, 0.05: 1.96, 0.10: 1.645}\n        z_score = z_map.get(alpha, 1.96)\n        \n        se = np.sqrt((sample_ps * (1 - sample_ps)) / n)\n        lower = sample_ps - (z_score * se)\n        upper = sample_ps + (z_score * se)\n        \n        hit = (lower &lt;= true_p) & (upper &gt;= true_p)\n        # Colors: Dark Grey for hits, Red for misses\n        colors = np.where(hit, '#555555', '#E74C3C')\n        widths = np.where(hit, 1, 2.5)\n        alphas = np.where(hit, 0.4, 1.0)\n        \n        fig, ax = plt.subplots(figsize=(10, 6), constrained_layout=True)\n        \n        # The Truth Line\n        ax.axvline(true_p, color='black', linestyle='-', linewidth=2, label=\"Truth\")\n        \n        y_pos = np.arange(N)\n        for i in range(N):\n            ax.hlines(y_pos[i], lower[i], upper[i], color=colors[i], linewidth=widths[i], alpha=alphas[i])\n        \n        ax.set_yticks([])\n        ax.set_xlim(0.2, 0.8)\n        \n        # Smaller Fonts\n        ax.set_xlabel(\"Estimated Proportion\", fontsize=10, color=\"gray\")\n        ax.set_title(f\"{N - np.sum(hit)} Intervals Missed the Truth\", fontsize=11, fontweight=\"bold\")\n        \n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        ax.spines['left'].set_visible(False)\n        ax.spines['bottom'].set_color('#CCCCCC')\n        ax.tick_params(axis='x', colors='gray')\n        \n        return fig\n\napp = App(app_ui, server)\n\nWe look at a Confidence Interval and our intuition sees a probability range for the truth. But the logic of orthodox statistics forbids this interpretation. It is a performance guarantee for a method, not a verdict on reality. Like the owls in Twin Peaks, the Log Lady warns us with the intervals."
  },
  {
    "objectID": "posts/learning_is_difficult/index.html",
    "href": "posts/learning_is_difficult/index.html",
    "title": "Learning is difficult",
    "section": "",
    "text": "Learning is difficult, and it doesn’t get easier. The only thing you realize is that the previous arduous steps seem surprisingly trivial in retrospect. This amazes me. It is like a paradox, the strenuous effort itself is painful and not immediately rewarding. You just remember the hardship as “huh, how hard this thing was before that I’m doing now as if nothing”. To keep learning and improving, you must have this awareness all the time, otherwise it’s just the feeling of difficulty.\nThis way of thinking lead me to understand that it is possible. When you encounter something new (a new topic or a skill), if it is very foreign to you, it may feel like it is impossible to comprehend or to achieve. But that is the only way. If it is difficult, I’m in the correct route. It may sound so trivial when I put it this way but we often miss how learning works and don’t recognize it when we avoid this hard effort. I remember when I meet with this peculiar exam for a pilot training program called DLR test. At first, I said this is just not possible for me. But day by day, improving incrementally, I passed it in the first try (later I gave up on this and thankfully not became a commercial pilot).\nI played video games since I was a kid. If you’d remember, games were more fun and they were objectively more difficult. I remember that I was grinding and pushing and keep trying when I was playing. It was just part of it. I remember that I was teaching techniques to neighbor’s kids older than me to pass a certain level in some game in Atari. Despite this childhood experience, I was struck with this famous indie game called Hollow Knight: Silksong. I said this is ridiculously hard, why do the developers torture me? I even thought some parts are just impossible for me to complete. But then, day by day you keep getting better and improve your skills. I’m amazed how parts that once made me want to throw the controller have now become so easy, only with my improvements in skills. There is the predominant advice in such games among the community: “git gud”.\n\n\n\nHornet from Silksong (Hollow Knight Fandom)\n\n\nI am not the most athletic type, maybe partly because of too much exercise in video games in my developing years and not so much in the sports. But tremendous news, it doesn’t matter too much if your goal is not become a pro. Even if you feel like inherently you are not good at something, or lack of some prior experience or talent, it is perfectly doable. I try to find things that are difficult and know that this is not only a good but the necessary indicator. At first, it was so painful to push myself in road cycling, but now I easily do a 100km in a day.\n\n\n\nMy shameless self promotion of my fitness\n\n\nWhat’s next is math. I’ve always felt inadequate. So my goal is to reach the proficiency level of a math undergrad to have what it takes to confidently discuss probability and statistics. It feels so difficult even now that it’s holding me back from starting. But let’s never ever forget that it is the only way to “git gud”."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Most published research findings are false (Ioannidis 2005). Unfortunately, the main reason isn’t a few “bad apples” engaging in p-hacking, data fabrication, or other fraudulent practices. Rather, it is due to a form of statistical blindness; specifically, the mechanical application of frequentist methods like significance testing simply because they are the industry standard.\nSince the early 2010s, we have indeed witnessed a massive issue known as the Replication Crisis. Many cornerstone studies, ranging from the social sciences to medicine, failed to replicate because they all followed the same rigid rules of statistical testing. Yet, we seem to keep applying temporary patches, such as lowering the acceptance threshold for p-values to 0.01.\nThe bridge out of this mess is actually rethinking probability at a fundamental level. It is a painful realization, mostly because it forces us to go back to the start while everyone else is still busy fighting reviewers for publications. But really, it just returns us to the perspective E.T. Jaynes pointed out: probability is not a tool for science, but the logic of science (Jaynes 2003). This isn’t something we can neglect, especially in the social sciences where statistical packages are too often treated like push-button appliances.\nSo, I decided to save the world (as every PhD student attempts to do) or die trying (which means overall, publishing a few papers). Bernoulli’s Fallacy by Aubrey Clayton (2021) greatly helped me put the pieces together. This blog documents my journey of following the Clayton’s steps, re-learning probability, running simulations, and building applets, along with some personal reflections."
  },
  {
    "objectID": "about.html#footnotes",
    "href": "about.html#footnotes",
    "title": "About",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(link to the ig page)↩︎"
  }
]